The basic idea behind PGM is that you have a partial matching, and you enlarge the matching one match by one match in each iterative step. The way you choose which possible match you add to the matchings you already have is by a sort of induction.

You already have your seed mapping as of now.

===pic of seed mapping

Those seed mappings should extend marks which mark each possible neighbor matching, saying that this one should be better or this one should be better:

===pic of seed mapping + matching, some seeds have now been "used"

At any point, if you get above a threshhold of marks for a possible matching, you can now say that the possible matching is a real one.

=== pic of new matching

In working Python code:

```
import collections
import random
import itertools
import networkx as nx

def normal_pgm(net1, net2, seeds, r):
    """
    net1 and net2 are networkx networks
    seeds is a list of correct seed matchings
    r is how many marks you need to say that a matching is good
    Gets about 1200 matches on 2048 SKG, selected 90%
    """
    marks = collections.defaultdict(int)
    impossible_tails, impossible_heads = set(), set()
    unused, used = seeds[:], []
    random.shuffle(unused) # mutation!
    while unused:
        curr_pair = unused.pop()
        prod_len = len(net1.neighbors(curr_pair[0])) * len(net2.neighbors(curr_pair[0]))
        for neighbor in itertools.product(net1.neighbors(curr_pair[0]), net2.neighbors(curr_pair[1])):
            if neighbor[0] in impossible_heads or neighbor[1] in impossible_tails:
                continue
            marks[neighbor] += 1
            if marks[neighbor] > r:
                unused.append(neighbor)
                impossible_heads.add(neighbor[0])
                impossible_tails.add(neighbor[1])
                continue
        used.append(curr_pair)
    return used
```

There are a few variants which exist, of which I used one which separates the matching itself and the possibility of spreading the marks: see the repository for details.

The "percolation" referred to in the percolation matching indicates the percolation of marks throughout the mapping and the way that the mapping expands one by one. This also is the main cause of why the algorithm is of interest to me, because it is why the algorithm creates long-distance influence in the matchings.

What do I mean by long-distance influence? If you have a new matching, that matching was deemed to be a new matching because it had _r_ marks, which came in and of themselves from older matchings. So information (marks) from matchings can carry over a very long way indeed sometimes, in that a cascade might result (a formal percolation on the network, or a network phase transition) where one new matching tips over one possible new matching which almost had enough marks, which in turn tips over another, towards criticality, just like how nuclear criticality works. There is a much more rigorous and fairly clear and easy treatment of this in the paper that also clearly put forth the idea of the percolation graph matching as a basic algorithm (not a heuristic, which is how A. Narayanan used it).

This also reminds me of a strange comment by J. Crutchfield about information:
https://www.youtube.com/watch?v=m-Jk1R26p6Q

So there's a question of how these views are related, how is energy related to information. And they're just simultaneous and complementary accountings of the same phenomenon. So in some sense, there has to be some relationship. But I think the first fair thing to do is not assume anything at the outset... The first claim is, let's not conflate the two things. In fact, there are many arenas in which there is a conflation between concepts of energy and information.

For example, just to pick one, in machine learning, there are many interesting techniques where there's an analogy that people draw between changing parameters in a model and having the model come to equilibrium with the data when it finally describes it well or up to some error. And that is actually a conflation of these two concepts, if you actually look at the mathematics, you will see information on one hand, probabilities, and of course, minus log of probabilities, that's information, and that's related to an energy and people now think of learning as moving down- minimizing some energy on a surface. So this is actually sort of a non-trivial statement up here. Depending on the context, that's either outrageous or obvious.

So, let's go back to this:

<Shows a rube goldberg machine>

Here's a very physical system, you're looking at energy, but of course, how this chain has set up has to be very delicately poised at a number of these points such that not very much energy is actually transmitted. Nonetheless, as you go down the chain, there are huge effects happening, huge relative to how much energy is transmitted to one contingent stage to another.

For me, the first thing to say about it is that there are all these stage, like 30 of them, all very delicately designed, can't remember the number of takes they had to do to get that to work right, but each of these contingent stages transmits only a fraction of, say, kinetic or gravitational energy, whatever it is. They're different things. And presumably, we could all go through the movie and figure out where the energy is stored, how it's being transduced to the next stage. But if there's thirty stages, and if you're designing each of these contingencies to transmit just the smallest amount, say one tenth, then you get very suspicious of this energy accounting as an explanation for the car rolling off. I mean, come on, that car weighs 3500 pounds and it rolls off and stops, right? And there's friction and everything - how much of the initial little tweak that the off-screen person gave - that initial energy - is responsible for rolling a 3500-pound car off of a ramp? Well, if every of the 30 stages only had 1/10 of the amount of [energy], that's 1/10^30.

I'm pretty sure that actually Crutchfield did have a lot of the problems of machine learning in mind when talking about this, because that's a pretty great physical demonstration of the old vanishing gradient problem in deep and/or recurrent neural networks. His reaction to it as a chaotician is to note chaotic and information theoretic properties of it, which is interesting.
