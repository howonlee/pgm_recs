\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\newcommand{\del}{\nabla}
\begin{document}

\title{Poking at Black Swans with Percolation}
\author{Howon Lee}
\maketitle

What's original in this essay is not mine and what is mine is not original. I hope I've sourced everything enough so that you can also go through where I learned this and learn it yourself, because learning is a positive feedback process.

Consider a social network, whether electronic, like Facebook or real, like the one at your school or workplace. The electronic ones are better studied because they are easier to study and there's more money in it. The folks who study social networks tend to study them as mathematical objects. More specifically, they study social networks as graphs, with nodes and edges, where nodes are people and edges are relations between people. And wherever they have seen social networks, they have seen some strange differences with respect to random networks.

\begin{enumerate}
  \item They have seen a radical inequality in the distribution of the degrees (number of edges adjacent on each node) of the nodes, so that some nodes have a much larger proportion of the edges than others: sometimes this is called a fat tail on the degree distribution (sometimes alleged to be a power law, although that is more difficult to show).
  \item There are many more triangles in the graphs than would be expected at random, so if I know that you are a friend of a friend, then it is much more likely than random that I am a friend of you, too. The same is true for a lot of more complicated small shapes.
  \item It is much easier for any agent that spreads infectiously (pathogens, memes, cultural norms, rumors, etc) to spread than would happen in a random graph.
  \item Because it is so easy to make a connection between any two huge communities and therefore glomp them together into one really huge community, there tends to come about a giant component to the graph which always has a path between any two nodes.
\end{enumerate}

There are many others besides, but I think these are the only ones we need in this discussion. Very many networks besides also have these properties, but social networks are the best studied because there's a lot of money in studying social networks and they're really interesting.

Given these properties, many theorists have thought deeply of the underlying generative process that creates them and possibly model them and their properties, creating a menagerie of possible models to choose from. Of those graph models which attempt to model the strange properties of the social networks, all of them depend in some way on a positive feedback process for network creation. For example, the Barabasi-Albert model starts off with a tiny fully connected network, and adds nodes to the network one at a time following the principle of Matthew, or of preferential attachment: "to those who have much, more will be given". That is, the probability a new node is connected to one of the old nodes is proportional to the number of nodes which already exist. 

The same is also the case for the Stochastic Kronecker graph, which consists of looking at a possible network as an adjacency matrix (instead of merely a set of nodes and edges: two views of the same thing), drawing a stochastic fractal on the adjacency matrix, and looking at it as a set of nodes and edges again. Fractals have long been known to also be governed by the Matthew effect: look at the bottom edge of a Sierpinski triangle, which is almost all filled up because it was almost all filled up in a previous iteration of the expanding Sierpinski triangle.

=== picture here

Now, I noted before that a radical inequality exists between the number of edges in each node of the network, where some nodes have very many edges and some other nodes have very few. This idea is actually quite well-popularized and familiar: it goes often by the name of the Black Swan, which says that very rare events in time have a disproportionate effect on events, and are often inappropriately rationalized. To describe these distributions of events, one must resort to a better-descriptive set of laws and distributions, fractal analysis, power law analysis, and nonparametric statistics.

To use NN Taleb's terminology, social networks are something that definitively belongs to Extremistan, because there is no process within them driving them towards a normative standard. Because they are connected as they are, radical things can happen in them in a very short amount of time - revolutions and revolts and grand social movements happen - not often, but more than people think they do - but the vast majority of social movements are tiny and sputter out.

You can also tell by the strange behavior of averages. If I was in a room with a professional basketball player and you were to calculate the average height of the two people in the room, you would get something that wasn't far from the average height of the human population: this is Mediocrestan. If I was in a room with B. Obama and you were to calculate the average number of people-who-know-about-this-person in this room of two people, you would get a number that was very far from the average in the human population indeed.

The black swan is often noted to be difficult or impossible to deal with computationally and statistically. NN Taleb advocates merely dealing with the possible effects of the black swan, but this piqued me intellectually, because there exists some pretty great tools to deal with, computationally and statistically, large social networks. The most interesting of them, I think, is percolation graph matching, which was first invented and used to de-anonymize social networks by A. Narayanan.

De-anonymization is defined this way. You have an actual network and two graphs which look pretty similar which you are told that are samples from the larger actual network. You have to match up nodes in the two graphs so that they correspond to the same node in the actual network. This is a very dire business doing it with real social networks. The reason why it is interesting is that it is, like many NP-complete problems, much easier than you get told in school that it is. See here as to why. The natural thought to have to listening to how it works is that \emph{it seems somewhat limited in scope}, but I do not think this is the case, because you can match a real complex network up to a generated one on the other side of the matching.

The following is a tutorial on percolation graph matching. I also have ready and working code in a repo here.

-- tutorial goes here

I do not really know how far you could actually apply this. Power laws and more generally, radical inequalities in distribution are pretty ubiquitous in natural phenomena, because they are the natural result of any positive feedback mechanism, and positive feedback is as common as negative feedback in nature. Many financial signals are governed by power law-distributed jumps, for example. The power spectra of many turbulent fluid signals are of a power law nature. In addition to social networks, there are software networks, biological networks, and economic networks which have parts and pieces of the strange patterns and phenomena that govern social networks. But translating the spectral data and time series data to a coherent network representation is a pretty dire task, more difficult than ordinary discretization.

I read one day a very strange (but evidenced) speculation by G. Buzsaki, who noted that radically unequal distributions are pretty common in human cognition. This, according to him, is because there are heavily skewed distributions inherent in human reward discounting, in neural synaptic contact number, in firing rates of individual neurons, in synchronous discharge of neural populations, and in perception at a much higher level (Weber's law). I note that it may be possible to fit together the percolation graph matching with, say, a Boltzmann machine or multilayer perceptron core: it is certainly interesting enough for me to go and do that next. I do not think that graph matching is such a fundamentally different problem from soft constraint satisfaction, which both the Boltzmann machine and multilayer perceptron was originally invented to solve.

Email me at hlee.howon at gmail.com if you wish to ask me questions or harangue me about any corrections. Thanks to JB for reading drafts.

\end{document}

